{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9731046,"sourceType":"datasetVersion","datasetId":5780082},{"sourceId":146603,"sourceType":"modelInstanceVersion","modelInstanceId":124346,"modelId":130791},{"sourceId":85986,"sourceType":"modelInstanceVersion","modelInstanceId":72246,"modelId":78150}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n","metadata":{}},{"cell_type":"markdown","source":"This notebook is for:\n\n* Testing the finetuned gemma2_instruct_2b_en model published on Kaggle.\n* Uploading finetuned model on Huggingface.\n* Building a chatbot with the the finetuned model by experimenting with different methods.\n* Expanding model functionality by providing external reference context to support the LLM in generating factual response.","metadata":{}},{"cell_type":"markdown","source":"## load necessary packages","metadata":{}},{"cell_type":"code","source":"# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n!pip install -q -U keras-nlp\n!pip install -q -U keras>=3\n!pip install -q -U huggingface_hub","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-10T23:05:54.252443Z","iopub.execute_input":"2024-11-10T23:05:54.253896Z","iopub.status.idle":"2024-11-10T23:06:45.342036Z","shell.execute_reply.started":"2024-11-10T23:05:54.253816Z","shell.execute_reply":"2024-11-10T23:06:45.340168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n\n%pip install langchain langchain-community langchain-google-vertexai\n%pip install langchainhub\n%pip install langchain-groq\n%pip install gradio","metadata":{"execution":{"iopub.status.busy":"2024-11-10T23:07:39.619030Z","iopub.execute_input":"2024-11-10T23:07:39.620039Z","iopub.status.idle":"2024-11-10T23:09:33.466758Z","shell.execute_reply.started":"2024-11-10T23:07:39.619972Z","shell.execute_reply":"2024-11-10T23:09:33.464665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install PyMuPDF sentence-transformers langchain chromadb huggingface-hub\n!pip install langchain_core python-docx","metadata":{"execution":{"iopub.status.busy":"2024-11-10T23:10:37.221632Z","iopub.execute_input":"2024-11-10T23:10:37.222155Z","iopub.status.idle":"2024-11-10T23:11:40.524401Z","shell.execute_reply.started":"2024-11-10T23:10:37.222105Z","shell.execute_reply":"2024-11-10T23:11:40.523047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pip install --upgrade -q langchain langchain-google-vertexai","metadata":{"execution":{"iopub.status.busy":"2024-10-23T23:32:59.539529Z","iopub.execute_input":"2024-10-23T23:32:59.540012Z","iopub.status.idle":"2024-10-23T23:32:59.546005Z","shell.execute_reply.started":"2024-10-23T23:32:59.539952Z","shell.execute_reply":"2024-10-23T23:32:59.544754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Markdown\nimport textwrap\n\nimport os\nimport keras\nimport keras_nlp\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import HuggingFacePipeline\nfrom docx import Document","metadata":{"execution":{"iopub.status.busy":"2024-11-10T23:12:46.917537Z","iopub.execute_input":"2024-11-10T23:12:46.918017Z","iopub.status.idle":"2024-11-10T23:13:03.985607Z","shell.execute_reply.started":"2024-11-10T23:12:46.917971Z","shell.execute_reply":"2024-11-10T23:13:03.984313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n# Avoid memory fragmentation on JAX backend.\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\"","metadata":{"execution":{"iopub.status.busy":"2024-11-10T23:13:08.026124Z","iopub.execute_input":"2024-11-10T23:13:08.026915Z","iopub.status.idle":"2024-11-10T23:13:08.034763Z","shell.execute_reply.started":"2024-11-10T23:13:08.026865Z","shell.execute_reply":"2024-11-10T23:13:08.032955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving finetuned model variant on Huggingface","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"hftoken\")","metadata":{"execution":{"iopub.status.busy":"2024-11-10T10:47:41.404940Z","iopub.execute_input":"2024-11-10T10:47:41.405400Z","iopub.status.idle":"2024-11-10T10:47:41.611927Z","shell.execute_reply.started":"2024-11-10T10:47:41.405358Z","shell.execute_reply":"2024-11-10T10:47:41.610822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(secret_value_0)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T10:47:47.518921Z","iopub.execute_input":"2024-11-10T10:47:47.520005Z","iopub.status.idle":"2024-11-10T10:47:48.162919Z","shell.execute_reply.started":"2024-11-10T10:47:47.519957Z","shell.execute_reply":"2024-11-10T10:47:48.161649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U huggingface_hub","metadata":{"execution":{"iopub.status.busy":"2024-11-10T10:49:58.605818Z","iopub.execute_input":"2024-11-10T10:49:58.606328Z","iopub.status.idle":"2024-11-10T10:50:12.285613Z","shell.execute_reply.started":"2024-11-10T10:49:58.606256Z","shell.execute_reply":"2024-11-10T10:50:12.284254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import create_repo\n\nrepo_name = \"ddi-finetuned-gemma2\"\ncreate_repo(repo_name, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-05T12:44:44.662351Z","iopub.execute_input":"2024-11-05T12:44:44.663295Z","iopub.status.idle":"2024-11-05T12:44:45.880212Z","shell.execute_reply.started":"2024-11-05T12:44:44.663243Z","shell.execute_reply":"2024-11-05T12:44:45.879028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import upload_folder\n\n# Upload entire model directory to Hugging Face\nupload_folder(\n    repo_id=\"rukayatadedeji/ddi-finetuned-gemma2\",\n    folder_path=\"/kaggle/input/bert/keras/ddi_gemma2_e10/1\",\n    commit_message=\"Upload ddi finetuned Gemma2 model package\"\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-05T12:46:01.688120Z","iopub.execute_input":"2024-11-05T12:46:01.689072Z","iopub.status.idle":"2024-11-05T12:53:29.577583Z","shell.execute_reply.started":"2024-11-05T12:46:01.689024Z","shell.execute_reply":"2024-11-05T12:53:29.576088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define helper function\n\ndef display_chat(prompt, response):\n  '''Displays an LLM prompt and response in a pretty way.'''\n  prompt = prompt.replace('\\n\\n','<br><br>')\n  prompt = prompt.replace('\\n','<br>')\n  formatted_prompt = \"<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>\" + prompt + \"</blockquote></font>\"\n  response = response.replace('‚Ä¢', '  *')\n  response = textwrap.indent(response, '', predicate=lambda _: True)\n  response = response.replace('\\n\\n','<br><br>')\n  response = response.replace('\\n','<br>')\n  response = response.replace(\"```\",\"\")\n  formatted_text = \"<font size='+1' color='teal'>ü§ñ<blockquote>\" + response + \"</blockquote></font>\"\n  return Markdown(formatted_prompt+formatted_text)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T23:17:10.732615Z","iopub.execute_input":"2024-11-10T23:17:10.733663Z","iopub.status.idle":"2024-11-10T23:17:10.769082Z","shell.execute_reply.started":"2024-11-10T23:17:10.733468Z","shell.execute_reply":"2024-11-10T23:17:10.763840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using Langchain for Chatbot development","metadata":{}},{"cell_type":"code","source":"from langchain_google_vertexai import GemmaLocalKaggle\nllm = GemmaLocalKaggle(model_name=\"/kaggle/input/bert/keras/ddi_gemma2/1\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T15:58:35.724318Z","iopub.execute_input":"2024-10-24T15:58:35.724805Z","iopub.status.idle":"2024-10-24T16:01:02.629997Z","shell.execute_reply.started":"2024-10-24T15:58:35.724767Z","shell.execute_reply":"2024-10-24T16:01:02.627301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = llm.invoke(\"Can I take fluconazole with my simvastatin medication?\", max_tokens=300)\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:01:42.992718Z","iopub.execute_input":"2024-10-24T16:01:42.993356Z","iopub.status.idle":"2024-10-24T16:02:54.089050Z","shell.execute_reply.started":"2024-10-24T16:01:42.993297Z","shell.execute_reply":"2024-10-24T16:02:54.087355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain_google_vertexai import GemmaChatLocalKaggle\nllm = GemmaChatLocalKaggle(model_name=\"/kaggle/input/bert/keras/ddi_gemma2/1\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T23:41:20.464719Z","iopub.execute_input":"2024-10-24T23:41:20.465173Z","iopub.status.idle":"2024-10-24T23:44:22.033838Z","shell.execute_reply.started":"2024-10-24T23:41:20.465128Z","shell.execute_reply":"2024-10-24T23:44:22.032595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain_core.messages import (\n    HumanMessage\n)\n\nmessage1 = HumanMessage(content=\"Hi! Can I take fluconazole with my simvastatin medication?\")\nanswer1 = llm.invoke([message1], max_tokens=300, parse_response=True)\nprint(answer1)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:05:36.823971Z","iopub.execute_input":"2024-10-24T16:05:36.824507Z","iopub.status.idle":"2024-10-24T16:07:08.740025Z","shell.execute_reply.started":"2024-10-24T16:05:36.824461Z","shell.execute_reply":"2024-10-24T16:07:08.738662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"message2 = HumanMessage(content=\"Which drugs would result in severe adverse effect when used with Goserelin?\")\nanswer2 = llm.invoke([message1, answer1, message2], max_tokens=600, parse_response=True)\n\nprint(answer2)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:07:49.651479Z","iopub.execute_input":"2024-10-24T16:07:49.651953Z","iopub.status.idle":"2024-10-24T16:11:55.088005Z","shell.execute_reply.started":"2024-10-24T16:07:49.651914Z","shell.execute_reply":"2024-10-24T16:11:55.086338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.prompts import ChatPromptTemplate\nfrom langchain.schema.output_parser import StrOutputParser","metadata":{"execution":{"iopub.status.busy":"2024-10-24T23:45:32.231799Z","iopub.execute_input":"2024-10-24T23:45:32.232300Z","iopub.status.idle":"2024-10-24T23:45:32.241919Z","shell.execute_reply.started":"2024-10-24T23:45:32.232255Z","shell.execute_reply":"2024-10-24T23:45:32.240723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"template = (\"\"\"\n    You are a highly knowledgeable drug information assistant specializing in drug interactions enquiries.\n    Your goal is to provide an accurate response and relevant information on question delimited by triple backticks.\n    If you don't know the answer, honestly respond that you don't have the information. Avoid guessing or providing incomplete information.\n    \n    Example:\n    Instruction: Can I take warfarin with ibuprofen?\n    Response: Warfarin and ibuprofen can interact and increase the risk of bleeding. Ibuprofen is a drug with antiplatelet properties and may increase anticoagulation effect of warfarin. It is recommended to avoid using them together or consult your healthcare provider for alternatives.\n\n    Instruction: ```{instruction}```\n    \n    Response:\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T23:45:49.882566Z","iopub.execute_input":"2024-10-24T23:45:49.883003Z","iopub.status.idle":"2024-10-24T23:45:49.889456Z","shell.execute_reply.started":"2024-10-24T23:45:49.882962Z","shell.execute_reply":"2024-10-24T23:45:49.888246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = ChatPromptTemplate.from_template(template)\noutput_parser = StrOutputParser()\n\nchain = prompt | llm | output_parser","metadata":{"execution":{"iopub.status.busy":"2024-10-24T23:46:09.086064Z","iopub.execute_input":"2024-10-24T23:46:09.086558Z","iopub.status.idle":"2024-10-24T23:46:09.092819Z","shell.execute_reply.started":"2024-10-24T23:46:09.086493Z","shell.execute_reply":"2024-10-24T23:46:09.091596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"response = chain.invoke(\"Can I take fluconazole with my simvastatin medication?\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T23:46:20.953040Z","iopub.execute_input":"2024-10-24T23:46:20.953511Z","iopub.status.idle":"2024-10-24T23:47:18.386426Z","shell.execute_reply.started":"2024-10-24T23:46:20.953469Z","shell.execute_reply":"2024-10-24T23:47:18.385192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Markdown(response)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T23:47:59.770752Z","iopub.execute_input":"2024-10-24T23:47:59.771237Z","iopub.status.idle":"2024-10-24T23:47:59.780190Z","shell.execute_reply.started":"2024-10-24T23:47:59.771190Z","shell.execute_reply":"2024-10-24T23:47:59.778987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.chains import ConversationChain\nfrom langchain.chains.conversation.memory import ConversationSummaryMemory","metadata":{"execution":{"iopub.status.busy":"2024-10-24T23:48:10.965403Z","iopub.execute_input":"2024-10-24T23:48:10.966680Z","iopub.status.idle":"2024-10-24T23:48:11.094875Z","shell.execute_reply.started":"2024-10-24T23:48:10.966632Z","shell.execute_reply":"2024-10-24T23:48:11.093875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"template1='''The following is a friendly conversation between a human and an AI.\nThe AI is a highly knowledgeable drug information assistant specializing in drug interactions enquiries.\nThe AI's goal is to provide an accurate response and relevant information on question.\nIf the AI does not know the answer to a question, it truthfully says it does not know.\n\n\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:'''","metadata":{"execution":{"iopub.status.busy":"2024-10-24T23:48:18.813235Z","iopub.execute_input":"2024-10-24T23:48:18.814295Z","iopub.status.idle":"2024-10-24T23:48:18.819408Z","shell.execute_reply.started":"2024-10-24T23:48:18.814237Z","shell.execute_reply":"2024-10-24T23:48:18.818293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate\nchat_prompt = PromptTemplate(input_variables=['history', 'input'], \n                            template=template1)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T23:48:36.465188Z","iopub.execute_input":"2024-10-24T23:48:36.466279Z","iopub.status.idle":"2024-10-24T23:48:36.471634Z","shell.execute_reply.started":"2024-10-24T23:48:36.466230Z","shell.execute_reply":"2024-10-24T23:48:36.470473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary_memory = ConversationSummaryMemory(llm=llm)\nconversation = ConversationChain(\n    llm=llm,\n    prompt=chat_prompt,\n    memory=summary_memory,\n    verbose=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T23:50:41.438745Z","iopub.execute_input":"2024-10-24T23:50:41.439220Z","iopub.status.idle":"2024-10-24T23:50:41.582903Z","shell.execute_reply.started":"2024-10-24T23:50:41.439177Z","shell.execute_reply":"2024-10-24T23:50:41.581429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conversation.predict(input='Can I take fluconazole with my simvastatin medication?')","metadata":{"execution":{"iopub.status.busy":"2024-10-24T23:51:16.609301Z","iopub.execute_input":"2024-10-24T23:51:16.610567Z","iopub.status.idle":"2024-10-24T23:52:14.340851Z","shell.execute_reply.started":"2024-10-24T23:51:16.610515Z","shell.execute_reply":"2024-10-24T23:52:14.339775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary_memory.clear()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T23:57:46.865730Z","iopub.execute_input":"2024-10-24T23:57:46.866296Z","iopub.status.idle":"2024-10-24T23:57:46.872625Z","shell.execute_reply.started":"2024-10-24T23:57:46.866250Z","shell.execute_reply":"2024-10-24T23:57:46.871352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_query = 'Is it safe to take ibuprofen while on warfarin?'\nresponse = conversation.predict(input=input_query)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T23:57:53.747098Z","iopub.execute_input":"2024-10-24T23:57:53.747599Z","iopub.status.idle":"2024-10-24T23:58:39.362906Z","shell.execute_reply.started":"2024-10-24T23:57:53.747555Z","shell.execute_reply":"2024-10-24T23:58:39.361640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conversation.predict(input='Can I take artemether lumefantrine malaria drug with my vitamin c supplement?')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# A simple Chatbot","metadata":{}},{"cell_type":"code","source":"# Load the model\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"/kaggle/input/bert/keras/ddi_gemma2_e10/1\")\ngemma_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-11-10T23:13:27.645510Z","iopub.execute_input":"2024-11-10T23:13:27.646044Z","iopub.status.idle":"2024-11-10T23:15:31.093013Z","shell.execute_reply.started":"2024-11-10T23:13:27.645989Z","shell.execute_reply":"2024-11-10T23:15:31.091333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# sample the softmax probabilities of the model\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T23:16:45.687256Z","iopub.execute_input":"2024-11-10T23:16:45.687830Z","iopub.status.idle":"2024-11-10T23:16:45.714554Z","shell.execute_reply.started":"2024-11-10T23:16:45.687776Z","shell.execute_reply":"2024-11-10T23:16:45.712884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define a Chat class which maintains conversation history\nfrom https://ai.google.dev/gemma/docs/gemma_chat","metadata":{}},{"cell_type":"code","source":"class ChatState():\n  \"\"\"\n  Manages the conversation history for a turn-based chatbot\n  Follows the turn-based conversation guidelines for the Gemma family of models\n  documented at https://ai.google.dev/gemma/docs/formatting\n  \"\"\"\n\n  __START_TURN_USER__ = \"user\\n\"\n  __START_TURN_MODEL__ = \"model\\n\"\n  __END_TURN__ = \"\\n\"\n\n  def __init__(self, model, system=\"\"):\n    \"\"\"\n    Initializes the chat state.\n\n    Args:\n        model: The language model to use for generating responses.\n        system: (Optional) System instructions or bot description.\n    \"\"\"\n    self.model = model\n    self.system = system\n    self.history = []\n\n  def add_to_history_as_user(self, message):\n      \"\"\"\n      Adds a user message to the history with start/end turn markers.\n      \"\"\"\n      self.history.append(self.__START_TURN_USER__ + message + self.__END_TURN__)\n\n  def add_to_history_as_model(self, message):\n      \"\"\"\n      Adds a model response to the history with start/end turn markers.\n      \"\"\"\n      self.history.append(self.__START_TURN_MODEL__ + message + self.__END_TURN__)\n\n  def get_history(self):\n      \"\"\"\n      Returns the entire chat history as a single string.\n      \"\"\"\n      return \"\".join([*self.history])\n\n  def get_full_prompt(self):\n    \"\"\"\n    Builds the prompt for the language model, including history and system description.\n    \"\"\"\n    prompt = self.get_history() + self.__START_TURN_MODEL__\n    if len(self.system)>0:\n      prompt = self.system + \"\\n\" + prompt\n    return prompt\n\n  def send_message(self, message):\n    \"\"\"\n    Handles sending a user message and getting a model response.\n\n    Args:\n        message: The user's message.\n\n    Returns:\n        The model's response.\n    \"\"\"\n    self.add_to_history_as_user(message)\n    prompt = self.get_full_prompt()\n    response = self.model.generate(prompt, max_length=1024)\n    result = response.replace(prompt, \"\")  # Extract only the new response\n    self.add_to_history_as_model(result)\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-11-03T21:10:20.508324Z","iopub.execute_input":"2024-11-03T21:10:20.508843Z","iopub.status.idle":"2024-11-03T21:10:20.523030Z","shell.execute_reply.started":"2024-11-03T21:10:20.508796Z","shell.execute_reply":"2024-11-03T21:10:20.521776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the Chat object with the model\nchat = ChatState(gemma_lm)","metadata":{"execution":{"iopub.status.busy":"2024-11-03T21:10:41.479474Z","iopub.execute_input":"2024-11-03T21:10:41.479979Z","iopub.status.idle":"2024-11-03T21:10:41.485553Z","shell.execute_reply.started":"2024-11-03T21:10:41.479929Z","shell.execute_reply":"2024-11-03T21:10:41.484078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First prompt\nmessage = f\"Which drugs would result in severe adverse effect when used with Goserelin?\"\ndisplay_chat(message, chat.send_message(message))","metadata":{"execution":{"iopub.status.busy":"2024-10-27T18:39:16.130562Z","iopub.execute_input":"2024-10-27T18:39:16.131458Z","iopub.status.idle":"2024-10-27T18:39:59.114000Z","shell.execute_reply.started":"2024-10-27T18:39:16.131411Z","shell.execute_reply":"2024-10-27T18:39:59.113034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Send a follow-up prompt\nmessage = f\"Which drug are we discussing?\"\ndisplay_chat(message, chat.send_message(message))","metadata":{"execution":{"iopub.status.busy":"2024-10-27T18:40:09.122115Z","iopub.execute_input":"2024-10-27T18:40:09.123060Z","iopub.status.idle":"2024-10-27T18:40:11.451651Z","shell.execute_reply.started":"2024-10-27T18:40:09.123013Z","shell.execute_reply":"2024-10-27T18:40:11.450640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Second prompt\nmessage = f\"Can I take fluconazole with my simvastatin?\"\ndisplay_chat(message, chat.send_message(message))","metadata":{"execution":{"iopub.status.busy":"2024-10-27T18:40:32.148300Z","iopub.execute_input":"2024-10-27T18:40:32.148738Z","iopub.status.idle":"2024-10-27T18:40:37.101967Z","shell.execute_reply.started":"2024-10-27T18:40:32.148673Z","shell.execute_reply":"2024-10-27T18:40:37.101025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Will a simple system message improve Chatbot response?","metadata":{}},{"cell_type":"code","source":"template1='The following is a friendly conversation between a human and an AI Pharmacist.'","metadata":{"execution":{"iopub.status.busy":"2024-11-03T21:11:04.297779Z","iopub.execute_input":"2024-11-03T21:11:04.301266Z","iopub.status.idle":"2024-11-03T21:11:04.323484Z","shell.execute_reply.started":"2024-11-03T21:11:04.301059Z","shell.execute_reply":"2024-11-03T21:11:04.317999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the Chat object with the model and system message\nchat1 = ChatState(model=gemma_lm, system=template1)","metadata":{"execution":{"iopub.status.busy":"2024-11-03T21:11:08.789442Z","iopub.execute_input":"2024-11-03T21:11:08.790317Z","iopub.status.idle":"2024-11-03T21:11:08.795939Z","shell.execute_reply.started":"2024-11-03T21:11:08.790272Z","shell.execute_reply":"2024-11-03T21:11:08.794117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test impact of system message on chatbot\nmessage = f\"Which drugs would result in severe adverse effect when used with Goserelin?\"\ndisplay_chat(message, chat1.send_message(message))","metadata":{"execution":{"iopub.status.busy":"2024-10-27T18:41:15.882019Z","iopub.execute_input":"2024-10-27T18:41:15.882766Z","iopub.status.idle":"2024-10-27T18:41:22.411493Z","shell.execute_reply.started":"2024-10-27T18:41:15.882724Z","shell.execute_reply":"2024-10-27T18:41:22.410495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Send a follow-up prompt\nmessage = f\"I am currently taking fluoxetin for my depression and i also use carbamazepin for epilepsy. I am taking this goserelin to treat breast cancer?\"\ndisplay_chat(message, chat1.send_message(message))","metadata":{"execution":{"iopub.status.busy":"2024-10-27T18:41:46.591951Z","iopub.execute_input":"2024-10-27T18:41:46.592944Z","iopub.status.idle":"2024-10-27T18:41:51.835664Z","shell.execute_reply.started":"2024-10-27T18:41:46.592899Z","shell.execute_reply":"2024-10-27T18:41:51.834621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Send a follow-up prompt\nmessage = f\"Which drugs are we discussing?\"\ndisplay_chat(message, chat1.send_message(message))","metadata":{"execution":{"iopub.status.busy":"2024-10-27T18:42:03.263673Z","iopub.execute_input":"2024-10-27T18:42:03.264709Z","iopub.status.idle":"2024-10-27T18:42:07.611081Z","shell.execute_reply.started":"2024-10-27T18:42:03.264653Z","shell.execute_reply":"2024-10-27T18:42:07.610080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Send a follow-up prompt\nmessage = f\"Are you sure of the answer you provided about the interaction between goserelin with fluoxetin and goserilin with carbamazepine?\"\ndisplay_chat(message, chat1.send_message(message))","metadata":{"execution":{"iopub.status.busy":"2024-10-27T18:42:16.835147Z","iopub.execute_input":"2024-10-27T18:42:16.836084Z","iopub.status.idle":"2024-10-27T18:42:21.937912Z","shell.execute_reply.started":"2024-10-27T18:42:16.836040Z","shell.execute_reply":"2024-10-27T18:42:21.936928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Second prompt\nmessage = f\"Can I take fluconazole with my simvastatin?\"\ndisplay_chat(message, chat.send_message(message))","metadata":{"execution":{"iopub.status.busy":"2024-10-27T18:42:44.422640Z","iopub.execute_input":"2024-10-27T18:42:44.423044Z","iopub.status.idle":"2024-10-27T18:42:49.411667Z","shell.execute_reply.started":"2024-10-27T18:42:44.423007Z","shell.execute_reply":"2024-10-27T18:42:49.410525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implementing RAG","metadata":{}},{"cell_type":"code","source":"import fitz  # PyMuPDF\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import Chroma","metadata":{"execution":{"iopub.status.busy":"2024-10-26T21:47:06.654378Z","iopub.execute_input":"2024-10-26T21:47:06.654983Z","iopub.status.idle":"2024-10-26T21:47:07.634043Z","shell.execute_reply.started":"2024-10-26T21:47:06.654946Z","shell.execute_reply":"2024-10-26T21:47:07.633231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define some useful functions","metadata":{}},{"cell_type":"code","source":"# Load content from a PDF\ndef load_pdf(file_path):\n    pdf = fitz.open(file_path)\n    texts = []\n    for page_num in range(10, pdf.page_count - 214):\n        page = pdf[page_num]\n        text = page.get_text()\n        texts.append(text)\n    pdf.close()\n    return texts\n\n# Useful in our RAG implementation\nclass DocumentWithText:\n    def __init__(self, content, metadata=None):\n        self.page_content = content\n        self.metadata = metadata if metadata is not None else {}\n\n# Load and split context docx documents for RAG\ndef load_and_split_documents(file_path):\n    # Load the Word document\n    doc = Document(file_path)\n    documents = [DocumentWithText(paragraph.text) for paragraph in doc.paragraphs if paragraph.text]\n\n    # Here you can choose how to split the text\n    text_splitter = CharacterTextSplitter(chunk_size=9000, chunk_overlap=0)\n    texts = text_splitter.split_documents(documents)\n    return texts","metadata":{"execution":{"iopub.status.busy":"2024-10-26T21:47:20.521392Z","iopub.execute_input":"2024-10-26T21:47:20.521780Z","iopub.status.idle":"2024-10-26T21:47:20.530427Z","shell.execute_reply.started":"2024-10-26T21:47:20.521743Z","shell.execute_reply":"2024-10-26T21:47:20.529378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load the PDF and Chunk It As a RAG Database","metadata":{}},{"cell_type":"code","source":"pdf_texts = load_pdf(\"/kaggle/input/dd1-v1/Stockley_Drug Interactions.pdf\")\n\n# Split each page's text into chunks\nsplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\ndocuments = []\nfor page_text in pdf_texts:\n    page_chunks = splitter.split_text(page_text)\n    documents.extend(page_chunks)\n\nprint(\"documents len=\", len(documents), \"sample=\", documents[1])","metadata":{"execution":{"iopub.status.busy":"2024-10-26T21:47:33.470490Z","iopub.execute_input":"2024-10-26T21:47:33.470888Z","iopub.status.idle":"2024-10-26T21:47:43.083502Z","shell.execute_reply.started":"2024-10-26T21:47:33.470854Z","shell.execute_reply":"2024-10-26T21:47:43.082360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create embeddings from the documents and vector database retriever needed for the RAG-based chatbot","metadata":{}},{"cell_type":"code","source":"# Create special documents list required by Chroma\ntexts = [DocumentWithText(doc) for doc in documents]\n\n# Load the embeddings model\n# NOTE: You might need to experiment with different models\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Create a Chroma vector database from the documents\n# Important: Make sure to delete previous db (if any) or else retrieval returns lots of duplicates :)\ntry:\n  db.delete_collection()\nexcept:\n  pass\ndb = Chroma.from_documents(texts, embeddings, persist_directory=\"/kaggle/working/chroma_db\")","metadata":{"execution":{"iopub.status.busy":"2024-10-26T21:53:51.359872Z","iopub.execute_input":"2024-10-26T21:53:51.360301Z","iopub.status.idle":"2024-10-26T21:54:59.556455Z","shell.execute_reply.started":"2024-10-26T21:53:51.360261Z","shell.execute_reply":"2024-10-26T21:54:59.555601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Downloading the vector database for future use","metadata":{}},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working/chroma_db","metadata":{"execution":{"iopub.status.busy":"2024-10-26T22:05:43.506106Z","iopub.execute_input":"2024-10-26T22:05:43.506911Z","iopub.status.idle":"2024-10-26T22:05:52.843001Z","shell.execute_reply.started":"2024-10-26T22:05:43.506871Z","shell.execute_reply":"2024-10-26T22:05:52.841844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-10-26T22:07:57.444287Z","iopub.execute_input":"2024-10-26T22:07:57.445281Z","iopub.status.idle":"2024-10-26T22:07:58.517576Z","shell.execute_reply.started":"2024-10-26T22:07:57.445236Z","shell.execute_reply":"2024-10-26T22:07:58.516403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'file.zip')","metadata":{"execution":{"iopub.status.busy":"2024-10-26T22:09:02.723090Z","iopub.execute_input":"2024-10-26T22:09:02.724044Z","iopub.status.idle":"2024-10-26T22:09:02.732016Z","shell.execute_reply.started":"2024-10-26T22:09:02.723987Z","shell.execute_reply":"2024-10-26T22:09:02.731135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a retriever from the vector database\n# NOTE: You might need to experiment with retrieval parameters\n# search_type=\"similarity_score_threshold\", search_kwargs={\"k\": 3, \"score_threshold\": 0.2}\nretriever = db.as_retriever(search_type=\"mmr\", search_kwargs={'k': 2, 'fetch_k': 50})","metadata":{"execution":{"iopub.status.busy":"2024-10-26T22:40:46.998628Z","iopub.execute_input":"2024-10-26T22:40:46.999274Z","iopub.status.idle":"2024-10-26T22:40:47.003892Z","shell.execute_reply.started":"2024-10-26T22:40:46.999233Z","shell.execute_reply":"2024-10-26T22:40:47.002856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test the retriever on some 'prompts'","metadata":{}},{"cell_type":"code","source":"ret_docs = retriever.invoke(\"What is the interraction between ramipril and aliskiren interaction?\")\nprint(\"retreived\", len(ret_docs), \"documents\")\nprint(ret_docs)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T22:40:52.799089Z","iopub.execute_input":"2024-10-26T22:40:52.800078Z","iopub.status.idle":"2024-10-26T22:40:52.971160Z","shell.execute_reply.started":"2024-10-26T22:40:52.800024Z","shell.execute_reply":"2024-10-26T22:40:52.970103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Providing Context to the Chatbot","metadata":{}},{"cell_type":"code","source":"# reloading database for use in chatbot\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\n# Load the embeddings model\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\npersist_directory = \"/kaggle/input/dd1-v1/file/kaggle/working/chroma_db\"\nvector_store = Chroma(\n    persist_directory=persist_directory,\n    embedding_function=embeddings  # Use the same embedding function\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T23:17:33.798873Z","iopub.execute_input":"2024-11-10T23:17:33.799682Z","iopub.status.idle":"2024-11-10T23:17:50.082912Z","shell.execute_reply.started":"2024-11-10T23:17:33.799631Z","shell.execute_reply":"2024-11-10T23:17:50.081385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={'k': 2, 'fetch_k': 50})","metadata":{"execution":{"iopub.status.busy":"2024-11-10T23:19:37.536071Z","iopub.execute_input":"2024-11-10T23:19:37.537921Z","iopub.status.idle":"2024-11-10T23:19:37.544438Z","shell.execute_reply.started":"2024-11-10T23:19:37.537856Z","shell.execute_reply":"2024-11-10T23:19:37.542905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def retrieve_context_from_chroma(query, retriever):\n    \"\"\"\n    Retrieves relevant context from Chroma database based on the user's message.\n    \n    Args:\n        query: The user's message as the query.\n        client: The Chroma client to connect with the database.\n\n    Returns:\n        Relevant text from the Chroma database.\n    \"\"\"\n    # This searches for relevant documents within the Chroma database\n    results = retriever.invoke(query)\n    context_texts = [result.page_content for result in results]\n    return \"\\n\".join(context_texts)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T23:19:41.758857Z","iopub.execute_input":"2024-11-10T23:19:41.759393Z","iopub.status.idle":"2024-11-10T23:19:41.767251Z","shell.execute_reply.started":"2024-11-10T23:19:41.759347Z","shell.execute_reply":"2024-11-10T23:19:41.765664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Modified chat class for context\nclass ChatState():\n  \"\"\"\n  Manages the conversation history for a turn-based chatbot\n  Follows the turn-based conversation guidelines for the Gemma family of models\n  documented at https://ai.google.dev/gemma/docs/formatting\n  \"\"\"\n\n  __START_TURN_USER__ = \"user\\n\"\n  __START_TURN_MODEL__ = \"model\\n\"\n  __END_TURN__ = \"\\n\"\n\n  def __init__(self, model, system=\"\"):\n    \"\"\"\n    Initializes the chat state.\n\n    Args:\n        model: The language model to use for generating responses.\n        system: (Optional) System instructions or bot description.\n    \"\"\"\n    self.model = model\n    self.system = system\n    self.history = []\n\n  def add_to_history_as_user(self, message):\n      \"\"\"\n      Adds a user message to the history with start/end turn markers.\n      \"\"\"\n      self.history.append(self.__START_TURN_USER__ + message + self.__END_TURN__)\n\n  def add_to_history_as_model(self, message):\n      \"\"\"\n      Adds a model response to the history with start/end turn markers.\n      \"\"\"\n      self.history.append(self.__START_TURN_MODEL__ + message + self.__END_TURN__)\n\n  def get_history(self):\n      \"\"\"\n      Returns the entire chat history as a single string.\n      \"\"\"\n      return \"\".join([*self.history])\n\n  def get_full_prompt(self):\n    \"\"\"\n    Builds the prompt for the language model, including history and system description.\n    \"\"\"\n    prompt = self.get_history() + self.__START_TURN_MODEL__\n    if len(self.system)>0:\n      prompt = self.system + \"\\n\" + prompt\n    return prompt\n\n  def send_message(self, message):\n    \"\"\"\n    Handles sending a user message and getting a model response.\n\n    Args:\n        message: The user's message.\n\n    Returns:\n        The model's response.\n    \"\"\"\n    self.add_to_history_as_user(message)\n        \n    # Step 2: Retrieve context from Chroma\n    chroma_context = retrieve_context_from_chroma(message, retriever)\n        \n    # Step 3: Construct prompt with retrieved context\n    prompt = self.get_full_prompt()\n    full_prompt = chroma_context + \"\\n\\n\" + prompt\n        \n    # Generate response with full prompt\n    response = self.model.generate(full_prompt, max_length=1024)\n    result = response.replace(full_prompt, \"\")  # Extract only the new response\n        \n    # Add the result to chat history\n    self.add_to_history_as_model(result)\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-11-03T21:12:13.931816Z","iopub.execute_input":"2024-11-03T21:12:13.932373Z","iopub.status.idle":"2024-11-03T21:12:13.947269Z","shell.execute_reply.started":"2024-11-03T21:12:13.932327Z","shell.execute_reply":"2024-11-03T21:12:13.946075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test with your ChatState as before\nchat2 = ChatState(model=gemma_lm, system=template1)\nmessage = \"Is there an interaction between goserelin with carbamazepine?\"\ndisplay_chat(message, chat2.send_message(message))","metadata":{"execution":{"iopub.status.busy":"2024-11-03T21:12:20.712076Z","iopub.execute_input":"2024-11-03T21:12:20.713341Z","iopub.status.idle":"2024-11-03T21:15:32.705785Z","shell.execute_reply.started":"2024-11-03T21:12:20.713290Z","shell.execute_reply":"2024-11-03T21:15:32.704571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compare with retrieved context from vector store\nretrieve_context_from_chroma(message, retriever)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T20:59:40.043490Z","iopub.execute_input":"2024-10-27T20:59:40.043894Z","iopub.status.idle":"2024-10-27T20:59:41.534841Z","shell.execute_reply.started":"2024-10-27T20:59:40.043857Z","shell.execute_reply":"2024-10-27T20:59:41.533598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test with your ChatState as before\nchat2 = ChatState(model=gemma_lm, system=template1)\nmessage = \"Is there an interaction between goserelin with fluoxetin?\"\ndisplay_chat(message, chat2.send_message(message))","metadata":{"execution":{"iopub.status.busy":"2024-11-03T20:44:06.063922Z","iopub.execute_input":"2024-11-03T20:44:06.064379Z","iopub.status.idle":"2024-11-03T20:44:10.269782Z","shell.execute_reply.started":"2024-11-03T20:44:06.064334Z","shell.execute_reply":"2024-11-03T20:44:10.268741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compare with retrieved context from vector store\nretrieve_context_from_chroma(message, retriever)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T20:59:57.529295Z","iopub.execute_input":"2024-10-27T20:59:57.529995Z","iopub.status.idle":"2024-10-27T20:59:58.914585Z","shell.execute_reply.started":"2024-10-27T20:59:57.529954Z","shell.execute_reply":"2024-10-27T20:59:58.912433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chat2.send_message(message)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T21:00:07.770596Z","iopub.execute_input":"2024-10-27T21:00:07.771508Z","iopub.status.idle":"2024-10-27T21:00:13.285666Z","shell.execute_reply.started":"2024-10-27T21:00:07.771449Z","shell.execute_reply":"2024-10-27T21:00:13.284712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# A Simple User Interface","metadata":{}},{"cell_type":"code","source":"def chat_with_model(input, history):\n    '''Generates a response from the finetuned Gemma model'''\n    \n    answer = chat2.send_message(input)\n    response = {\"role\": \"assistant\", \"content\": \"\"}\n    response['content'] += answer\n    yield response","metadata":{"execution":{"iopub.status.busy":"2024-10-27T21:00:26.944461Z","iopub.execute_input":"2024-10-27T21:00:26.944851Z","iopub.status.idle":"2024-10-27T21:00:26.949906Z","shell.execute_reply.started":"2024-10-27T21:00:26.944815Z","shell.execute_reply":"2024-10-27T21:00:26.948971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Create a simple gradio chat interface and launch it\nimport gradio as gr\n# Launch the demo\ndemo = gr.ChatInterface(chat_with_model,\n                        type=\"messages\",\n                        description = \"Gemma-powered Drug Interactions AI App\")\ndemo.launch(share=True, debug=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T21:00:31.993035Z","iopub.execute_input":"2024-10-27T21:00:31.993805Z","iopub.status.idle":"2024-10-27T21:00:39.142417Z","shell.execute_reply.started":"2024-10-27T21:00:31.993763Z","shell.execute_reply":"2024-10-27T21:00:39.141274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Providing Chroma db and Web search context to Chatbot","metadata":{}},{"cell_type":"code","source":"! pip install -q -U tavily-python","metadata":{"execution":{"iopub.status.busy":"2024-11-10T23:20:02.817232Z","iopub.execute_input":"2024-11-10T23:20:02.817785Z","iopub.status.idle":"2024-11-10T23:20:19.591335Z","shell.execute_reply.started":"2024-11-10T23:20:02.817736Z","shell.execute_reply":"2024-11-10T23:20:19.589557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tavily import TavilyClient","metadata":{"execution":{"iopub.status.busy":"2024-11-10T23:20:46.286250Z","iopub.execute_input":"2024-11-10T23:20:46.287967Z","iopub.status.idle":"2024-11-10T23:20:46.310129Z","shell.execute_reply.started":"2024-11-10T23:20:46.287900Z","shell.execute_reply":"2024-11-10T23:20:46.308319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nsecret_label = \"tavily_key\"\napi_key = UserSecretsClient().get_secret(secret_label)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T23:20:50.585263Z","iopub.execute_input":"2024-11-10T23:20:50.586536Z","iopub.status.idle":"2024-11-10T23:20:50.793339Z","shell.execute_reply.started":"2024-11-10T23:20:50.586481Z","shell.execute_reply":"2024-11-10T23:20:50.791942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tavily = TavilyClient(api_key)","metadata":{"execution":{"iopub.status.busy":"2024-11-03T20:47:31.247755Z","iopub.execute_input":"2024-11-03T20:47:31.248409Z","iopub.status.idle":"2024-11-03T20:47:31.252678Z","shell.execute_reply.started":"2024-11-03T20:47:31.248370Z","shell.execute_reply":"2024-11-03T20:47:31.251752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query = \"Is there an interaction between Goserelin and fluoxetin?\"\nresponse = tavily.search(query, include_domains=['reference.medscape.com'])\nfor result in response['results']:\n    print(result['title'])\n    print(result['content'])","metadata":{"execution":{"iopub.status.busy":"2024-11-03T20:49:43.839494Z","iopub.execute_input":"2024-11-03T20:49:43.840558Z","iopub.status.idle":"2024-11-03T20:49:46.511482Z","shell.execute_reply.started":"2024-11-03T20:49:43.840509Z","shell.execute_reply":"2024-11-03T20:49:46.510356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(response)","metadata":{"execution":{"iopub.status.busy":"2024-11-03T20:50:33.085297Z","iopub.execute_input":"2024-11-03T20:50:33.086264Z","iopub.status.idle":"2024-11-03T20:50:33.091059Z","shell.execute_reply.started":"2024-11-03T20:50:33.086221Z","shell.execute_reply":"2024-11-03T20:50:33.090137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"search_api = TavilyClient(api_key)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T23:21:00.634481Z","iopub.execute_input":"2024-11-10T23:21:00.635252Z","iopub.status.idle":"2024-11-10T23:21:00.642097Z","shell.execute_reply.started":"2024-11-10T23:21:00.635162Z","shell.execute_reply":"2024-11-10T23:21:00.640417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def search_web(query, search_api):\n    \"\"\"\n    Performs a web search, prioritizing included domain(s) and retrieves the top result snippets.\n    \n    Args:\n        query: The search query.\n        search_api: The Tavily search client.\n    \n    Returns:\n        A string containing relevant text from the web search results.\n    \"\"\"\n    response = search_api.search(query, include_domains=['reference.medscape.com'])\n    snippets = [result['content'] for result in response['results']]\n    return \"\\n\".join(snippets)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T23:21:04.749084Z","iopub.execute_input":"2024-11-10T23:21:04.749661Z","iopub.status.idle":"2024-11-10T23:21:04.759081Z","shell.execute_reply.started":"2024-11-10T23:21:04.749611Z","shell.execute_reply":"2024-11-10T23:21:04.757176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Modified chat class for context and web result\nclass ChatState():\n  \"\"\"\n  Manages the conversation history for a turn-based chatbot\n  Follows the turn-based conversation guidelines for the Gemma family of models\n  documented at https://ai.google.dev/gemma/docs/formatting\n  \"\"\"\n\n  __START_TURN_USER__ = \"user\\n\"\n  __START_TURN_MODEL__ = \"model\\n\"\n  __END_TURN__ = \"\\n\"\n\n  def __init__(self, model, system=\"\"):\n    \"\"\"\n    Initializes the chat state.\n\n    Args:\n        model: The language model to use for generating responses.\n        system: (Optional) System instructions or bot description.\n    \"\"\"\n    self.model = model\n    self.system = system\n    self.history = []\n\n  def add_to_history_as_user(self, message):\n    \"\"\"\n    Adds a user message to the history with start/end turn markers.\n    \"\"\"\n    self.history.append(self.__START_TURN_USER__ + message + self.__END_TURN__)\n\n  def add_to_history_as_model(self, message):\n    \"\"\"\n    Adds a model response to the history with start/end turn markers.\n    \"\"\"\n    self.history.append(self.__START_TURN_MODEL__ + message + self.__END_TURN__)\n\n  def get_history(self):\n    \"\"\"\n    Returns the entire chat history as a single string.\n    \"\"\"\n    return \"\".join([*self.history])\n\n  def get_full_prompt(self):\n    \"\"\"\n    Builds the prompt for the language model, including history and system description.\n    \"\"\"\n    prompt = self.get_history() + self.__START_TURN_MODEL__\n    if len(self.system)>0:\n      prompt = self.system + \"\\n\" + prompt\n    return prompt\n\n  def send_message(self, message):\n    \"\"\"\n    Handles sending a user message and getting a model response.\n\n    Args:\n        message: The user's message.\n\n    Returns:\n        The model's response.\n    \"\"\"\n    self.add_to_history_as_user(message)\n        \n    # Step 2: Retrieve context from Chroma\n    chroma_context = retrieve_context_from_chroma(message, retriever)\n        \n    # Step 3: Retrieve web search context\n    web_context = search_web(message, search_api)\n        \n    # Step 4: Construct prompt with both Chroma and web search contexts\n    prompt = self.get_full_prompt()\n    full_prompt = f\"\"\"You are a highly knowledgeable drug information assistant specializing in drug interactions enquiries. \n                Your goal is to provide an accurate response and relevant information on question below. \n                If you don't know the answer, honestly respond that you don't have the information. Avoid guessing or providing incomplete information.\n\n                Here is an example:\n                Query: Can I take warfarin with ibuprofen?\n                Response: Warfarin and ibuprofen can interact and increase the risk of bleeding. Ibuprofen is a drug with antiplatelet properties and may increase anticoagulation effect of warfarin. It is recommended to avoid using them together or consult your healthcare provider for alternatives.\n\n                Use the following context to answer the question below:\n                {chroma_context}\n                {web_context}\n                \n                Use the following history of your interaction with the user when needed to help answer the question below:\n                {prompt}\n\n                Question:\n                {message}\n\n                Answer:\"\"\"\n        \n    # Generate response with full prompt\n    response = self.model.generate(full_prompt, max_length=1024)\n    result = response.replace(full_prompt, \"\")  # Extract only the new response\n        \n    # Add the result to chat history\n    self.add_to_history_as_model(result)\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-11-03T21:51:31.587790Z","iopub.execute_input":"2024-11-03T21:51:31.588298Z","iopub.status.idle":"2024-11-03T21:51:31.603881Z","shell.execute_reply.started":"2024-11-03T21:51:31.588252Z","shell.execute_reply":"2024-11-03T21:51:31.602340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Modified chat class for context and web result\nclass ChatState():\n  \"\"\"\n  Manages the conversation history for a turn-based chatbot\n  Follows the turn-based conversation guidelines for the Gemma family of models\n  documented at https://ai.google.dev/gemma/docs/formatting\n  \"\"\"\n\n  __START_TURN_USER__ = \"user\\n\"\n  __START_TURN_MODEL__ = \"model\\n\"\n  __END_TURN__ = \"\\n\"\n\n  def __init__(self, model, system=\"\"):\n    \"\"\"\n    Initializes the chat state.\n\n    Args:\n        model: The language model to use for generating responses.\n        system: (Optional) System instructions or bot description.\n    \"\"\"\n    self.model = model\n    self.system = system\n    self.history = []\n\n  def add_to_history_as_user(self, message):\n    \"\"\"\n    Adds a user message to the history with start/end turn markers.\n    \"\"\"\n    self.history.append(self.__START_TURN_USER__ + message + self.__END_TURN__)\n\n  def add_to_history_as_model(self, message):\n    \"\"\"\n    Adds a model response to the history with start/end turn markers.\n    \"\"\"\n    self.history.append(self.__START_TURN_MODEL__ + message + self.__END_TURN__)\n\n  def get_history(self):\n    \"\"\"\n    Returns the entire chat history as a single string.\n    \"\"\"\n    return \"\".join([*self.history])\n\n  def get_full_prompt(self):\n    \"\"\"\n    Builds the prompt for the language model, including history and system description.\n    \"\"\"\n    prompt = self.get_history() + self.__START_TURN_MODEL__\n    if len(self.system)>0:\n      prompt = self.system + \"\\n\" + prompt\n    return prompt\n\n  def send_message(self, message):\n    \"\"\"\n    Handles sending a user message and getting a model response.\n\n    Args:\n        message: The user's message.\n\n    Returns:\n        The model's response.\n    \"\"\"\n    self.add_to_history_as_user(message)\n        \n    # Step 2: Retrieve context from Chroma\n    chroma_context = retrieve_context_from_chroma(message, retriever)\n        \n    # Step 3: Retrieve web search context\n    web_context = search_web(message, search_api)\n        \n    # Step 4: Construct prompt with both Chroma and web search contexts\n    prompt = self.get_full_prompt()\n    full_prompt = f\"{chroma_context}\\n\\n{web_context}\\n\\n{prompt}\"\n        \n    # Generate response with full prompt\n    response = self.model.generate(full_prompt, max_length=1024)\n    result = response.replace(full_prompt, \"\")  # Extract only the new response\n        \n    # Add the result to chat history\n    self.add_to_history_as_model(result)\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-11-03T22:12:48.603420Z","iopub.execute_input":"2024-11-03T22:12:48.604511Z","iopub.status.idle":"2024-11-03T22:12:48.618523Z","shell.execute_reply.started":"2024-11-03T22:12:48.604460Z","shell.execute_reply":"2024-11-03T22:12:48.617178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trying Prompt template","metadata":{}},{"cell_type":"code","source":"## Correction from advisor\nclass ChatState():\n  \"\"\"\n  Manages the conversation history for a turn-based chatbot\n  Follows the turn-based conversation guidelines for the Gemma family of models\n  documented at https://ai.google.dev/gemma/docs/formatting\n  \"\"\"\n  __START_TURN_USER__ = \"user\\n\" # NOTE: This is only valid for gemma2_instr\n  __START_TURN_MODEL__ = \"model\\n\" # NOTE: This is only valid for gemma2_instr\n  __END_TURN__ = \"\\n\" # NOTE: This is only valid for gemma2_instr\n  def __init__(self, model, tokenizer=None, system=\"\"):\n    \"\"\"\n    Initializes the chat state.\n    Args:\n        model: The language model to use for generating responses.\n        system: (Optional) System instructions or bot description.\n    \"\"\"\n    self.model = model\n    self.tokenizer = tokenizer\n    self.system = system\n    self.history = []\n  def add_to_history_as_user(self, message):\n    \"\"\"\n    Adds a user message to the history with start/end turn markers.\n    \"\"\"\n    self.history.append(self.__START_TURN_USER__ + message + self.__END_TURN__)\n  def add_to_history_as_model(self, message):\n    \"\"\"\n    Adds a model response to the history with start/end turn markers.\n    \"\"\"\n    self.history.append(self.__START_TURN_MODEL__ + message ) #+ self.__END_TURN__)\n  def get_history(self):\n    \"\"\"\n    Returns the entire chat history as a single string.\n    \"\"\"\n    return \"\".join([*self.history])\n  def get_history_blurb(self):\n    \"\"\"\n    Returns what to insert into the current prompt\n    \"\"\"\n    if len(self.history)==0:\n      return \"\"\n    else:\n      return \\\nf\"\"\"\\n\\nUse the following history of your interaction with the user to help answer the question below:\\n\"\"\"\\\nf\"\"\"{self.get_history()}\"\"\"\n\n  def get_full_prompt(self):\n    \"\"\"\n    Builds the prompt for the language model, including history and system description.\n    \"\"\"\n    prompt = self.get_history() + self.__START_TURN_MODEL__\n    if len(self.system)>0:\n      prompt = self.system + \"\\n\" + prompt\n    return prompt\n  def send_message(self, message):\n    \"\"\"\n    Handles sending a user message and getting a model response.\n    Args:\n        message: The user's message.\n    Returns:\n        The model's response.\n    \"\"\"\n    # Step 2: Fake retrieving context from Chroma\n    chroma_context = retrieve_context_from_chroma(message, retriever)\n    # Step 3: Fake retrieving web search context\n    web_context = search_web(message, search_api)\n    # Step 4: Construct prompt with both Chroma and web search contexts\n    prompt = self.get_full_prompt()\n    full_prompt = \\\nf\"\"\"You are a highly knowledgeable drug information assistant specializing in drug interactions enquiries. \"\"\"\\\nf\"\"\"Your goal is to provide an accurate response and relevant information on question below. \"\"\"\\\nf\"\"\"If you don't know the answer, honestly respond that you don't have the information. \"\"\"\\\nf\"\"\"Avoid guessing or providing incomplete information.\\n\\n\"\"\"\\\nf\"\"\"Here is an example:\\n\"\"\"\\\nf\"\"\"Query: Can I take warfarin with ibuprofen?\\n\"\"\"\\\nf\"\"\"Response: Warfarin and ibuprofen can interact and increase the risk of bleeding. \"\"\"\\\nf\"\"\"Ibuprofen is a drug with antiplatelet properties and may increase anticoagulation effect of warfarin. \"\"\"\\\nf\"\"\"It is recommended to avoid using them together or consult your healthcare provider for alternatives.\\n\\n\"\"\"\\\nf\"\"\"Use the following context to answer the question below:\\n\"\"\"\\\nf\"\"\"{chroma_context}\"\"\"\\\nf\"\"\"{web_context}\"\"\"\\\nf\"\"\"{self.get_history_blurb()}\"\"\"\\\nf\"\"\"\\n{self.__START_TURN_USER__ }\"\"\"\\\nf\"\"\"{message}\"\"\"\\\nf\"\"\"\\n{self.__END_TURN__ }\"\"\"\\\nf\"\"\"\\n{self.__START_TURN_MODEL__ }\"\"\"\n    # for debugging - \n    print(\"--->\\n\" + full_prompt + \"<--\")\n    tokenized_input = tokenizer.tokenize(full_prompt)\n    print(\"PROMPT NUM TOKENS=\", len(tokenized_input))\n    self.add_to_history_as_user(message)\n    # Generate response with full prompt\n    response = self.model.generate(full_prompt, max_length=1024)\n    # for debugging - \n    print(\"--->\\n\" + response + \"<--\")\n    result = response.replace(full_prompt, \"\")  # Extract only the new response\n    # Add the result to chat history\n    self.add_to_history_as_model(result)\n\n    return result\nprint(\"Done\")","metadata":{"execution":{"iopub.status.busy":"2024-11-10T23:38:11.466309Z","iopub.execute_input":"2024-11-10T23:38:11.467404Z","iopub.status.idle":"2024-11-10T23:38:11.487112Z","shell.execute_reply.started":"2024-11-10T23:38:11.467347Z","shell.execute_reply":"2024-11-10T23:38:11.485682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test your enhanced ChatState\ntokenizer = keras_nlp.models.GemmaTokenizer.from_preset(\"gemma2_instruct_2b_en\")\nchat2 = ChatState(model=gemma_lm, tokenizer=tokenizer)\nprint(\"Done\")","metadata":{"execution":{"iopub.status.busy":"2024-11-10T23:38:16.106809Z","iopub.execute_input":"2024-11-10T23:38:16.107286Z","iopub.status.idle":"2024-11-10T23:38:19.063272Z","shell.execute_reply.started":"2024-11-10T23:38:16.107239Z","shell.execute_reply":"2024-11-10T23:38:19.061907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"message = \"Is there an interaction between goserelin with carbamazepine?\"\ndisplay_chat(message, chat2.send_message(message))","metadata":{"execution":{"iopub.status.busy":"2024-11-10T23:38:26.888528Z","iopub.execute_input":"2024-11-10T23:38:26.889487Z","iopub.status.idle":"2024-11-10T23:39:10.392862Z","shell.execute_reply.started":"2024-11-10T23:38:26.889436Z","shell.execute_reply":"2024-11-10T23:39:10.391695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"message = \"What drugs are discussing?\"\ndisplay_chat(message, chat2.send_message(message))","metadata":{"execution":{"iopub.status.busy":"2024-11-10T23:39:33.231348Z","iopub.execute_input":"2024-11-10T23:39:33.231815Z","iopub.status.idle":"2024-11-10T23:40:16.984484Z","shell.execute_reply.started":"2024-11-10T23:39:33.231772Z","shell.execute_reply":"2024-11-10T23:40:16.983144Z"},"trusted":true},"execution_count":null,"outputs":[]}]}